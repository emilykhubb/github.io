%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
    \lstset{language=R}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm,amssymb} % Math packages
\usepackage{graphicx}
\usepackage{url}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\newcommand{\argmax}{\arg\!\max}
\title{	
\normalfont \normalsize 
\textsc{Mississippi State University} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Data Analysis One  \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Emily Hubbard} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section*{HW 2}
%\lipsum[2] % Dummy text

\textbf{Executive Summary:}\\

\textbf{A. Introduction:} \\
Linear Regression is a way to investigate whether or not two variables or more have a linear relationship. As one increases or decreases, does the other increase or decrease? When testing the importance of more than one covariate, the process is known as Multiple Linear Regression. This allows us to understand the effects of multiple variables on a specific response variable. Depending on the data, this can allow for a more precise model. The most crucial part of MLR is to find the correct covariates to consider. Linear Regression loses its usefulness if the model becomes too complicated. One of the motivations to us LR is to have an easy interpretation of coefficients. This characteristic becomes less impactful when there are too many variables to make reasonable interpretations. Therefore, choosing the most valuable covariates is the most important step in creating a MLR model. However, if this is done correctly, it can allow for a more well-rounded interpretation of covariates and prediction of the response variable. The same as simple LR, you test the reliability of the model based on its satisfaction of the base assumptions: 1. Linearity, 2. Normality, 3. Constant Variance, 4. Independence. \\

This data set includes 12 variables on 1,266 separate orders received in the month of May 2014 at an Italian restaraunt offering home delivery. This report will outline the process of finding the best fit MLR model, checking assumptions, and drawing impactful interpretations. \\

A simple linear model and a multiple linear model are based on the following formulas, respectively:
\begin{align}
\nonumber
&\text{Y} = \alpha +  \beta_1X + \epsilon \text{  (Simple LR)}\\
\nonumber
&\text{Y} = \alpha + \sum_{i=0}^{p} \beta_iX_i + \epsilon \text{ (Multiple LR)}\\
\nonumber
\end{align} 

\textbf{B. Data Collection}\\
Consider the pizza delivery data described below: The pizza delivery data is a simulated data set.
The data refers to an Italian restaurant which offers home delivery of pizza. It contains the orders
received during a period of one month: May 2014. There are three branches of the restaurant. The
pizza delivery is centrally managed: an operator receives a phone call and forwards the order to
the branch which is nearest to the customerâ€™s address. One of the five drivers (two of whom only
work part time at the weekend) delivers the order. The data set captures the number of pizzas
ordered as well as the final bill which may also include drinks, salads, and pasta dishes. The owner
of the business observed an increased number of complaints, mostly because pizzas arrive too late
and too cold. To improve the service quality of his business, the owner wants to measure:
\begin{itemize}
\item the time from call to delivery and
\item the pizza temperature at arrival (which can be done with a special device)
\end{itemize}
Ideally, a pizza arrives within 30 min of the call; if it takes longer than 40 min, then the customers
are promised a free bottle of water. The temperature of the pizza should be above $65^C$ at the time
of delivery. The analysis of the data aims to determine the factors which influence delivery time
and temperature of the pizzas.\\

\textbf{B. Summary Information}\\ 


The best fit model when considering two or more covariates (multiple regression) is given when running a stepAIC function on the response variable time given all covariates. This will then rule out any covariates that do not have a significant effect on the response variable. The covariates that were outputted by the stepAIC function were the following: temperature, branch, day, driver, bill, and number of pizzas. This model gives us the highest $R^2_{adj}$ compared to the first model tested in this report.\\ 

The $R^2_{adj}$ is still quite low, around 31 percent, but there is still a significant effect on the response given these 6 covariates. To clean this model, the next steps would be to consider combining or getting rid of some of the covariates due to correlation. It is also worth considering implementing some transformations on certain variables to see if that increases $R^2_{adj}$. Overall, this model needs work because as seen when predicting the response variable in question 10, it was not exact. There was at least 4 percent error. Whether this is good or bad, I would have to know more about the industry. However, I think there is potential to find a more precise model working with the same covariates that are altered in some way.\\

Now we will look at these results in more detail.\\

    \textbf{Data Analysis:}\\

\begin{enumerate}
    \item Read the data into R. Fit a multiple linear regression model with delivery time as the outcome
    and temperature, branch, day, operator, driver, bill, number of ordered pizzas, and discount
    customer as covariates. Give a summary of the coefficients. \\

\textbf{Covariate Coefficients:}\\
The covariates that are \textbf{bolded} are those that have significant p-values. This is to visually see the important covariates that we may want to consider keeping in the model. This will also offer us a comparison when we run the stepAIC function. 
\begin{align}
\nonumber
&\textbf{Temperature} = -0.208 \\
\nonumber
&\textbf{Branch East} = -1.603 \\
\nonumber
&\text{Branch West} = -0.119 \\
\nonumber
&\text{Day Monday} = -1.159 \\
\nonumber
&\text{Day Saturday} =  0.882 \\
\nonumber
&\text{Day Sunday} = 1.017\\
\nonumber
&\text{Day Thursday} = 0.789 \\
\nonumber
&\text{Day Tuesday} =  0.793 \\
\nonumber
&\text{Day Wednesday} = 0.258 \\
\nonumber
&\text{Operator Melissa} = -0.158 \\
\nonumber
&\textbf{Driver Domenico} = -2.593 \\
\nonumber
&\text{Driver Luigi} = -0.809 \\
\nonumber
&\text{Driver Mario} = -0.395 \\
\nonumber
&\text{Driver Salvatore} = -0.504 \\
\nonumber
&\textbf{Bill} = 0.141 \\
\nonumber
&\textbf{Pizzas} = 0.556 \\
\nonumber
&\text{Discount Customer} =  -0.283\\
\nonumber
\end{align} 

\textbf{Explanation:}\\ 
Using the summary output, it is easy to see the significance of some of the covariate coefficients in predicting pizza delivery time. Whenever the covariate is increased by one unit, its corresponding coefficient gives the increase(+) or decrease(-) that will be seen in pizza delivery time. There are 5 coefficients that are extremely significant in predicting the response variable: temperature, branchEast, driverDomenico, bill, and number of pizzas. According to the p-values (which are also significantly lower than an $\alpha=0.05$), these variables are important in this linear regression model. This is also supported by the p-value result of the F-test. Having such a low p-value indicates that the full model (as compared to the reduced model) is a better predictor for the response variable. What this means is that the covariates are important and at least some of them should be kept in the model to allow a more accurate prediction.\\

    \item Use R to calculate the 95 percent confidence intervals of all coefficients. \\

\textbf{Confidence Intervals:}
\begin{align}
\nonumber
&\textbf{Temperature} = (-0.259,-0.157) \\
\nonumber
&\textbf{Branch East} =  (-2.433,-0.772)\\
\nonumber
&\text{Branch West} = (-0.851,0.613)\\
\nonumber
&\text{Day Monday} = (-2.400,0.083) \\
\nonumber
&\text{Day Saturday} =  (-0.102,1.866)\\
\nonumber
&\text{Day Sunday} = (-0.084,2.117)\\
\nonumber
&\text{Day Thursday} = (-0.251,1.829) \\
\nonumber
&\text{Day Tuesday} =  (-0.434,2.020)\\
\nonumber
&\text{Day Wednesday} = (-0.932,1.448)\\
\nonumber
&\text{Operator Melissa} = (-0.831,0.515) \\
\nonumber
&\textbf{Driver Domenico} =  (-4.034,-1.152)\\
\nonumber
&\text{Driver Luigi} = (-1.961,0.343)\\
\nonumber
&\text{Driver Mario} = (-1.252,0.462) \\
\nonumber
&\text{Driver Salvatore} =  (-1.357,0.349)\\
\nonumber
&\textbf{Bill} = (0.110,0.172)\\
\nonumber
&\textbf{Pizzas} = (0.326,0.786) \\
\nonumber
&\text{Discount Customer} =  (-1.006,0.440)\\
\nonumber
\end{align}  

\textbf{Explanation:}\\ 
Notice the sign of the values within the confidence intervals. These values give us another indication of the most significant variables. All 5 that showed significant p-values also have a range that is fully positive or fully negative. This makes sense because when a confidence interval includes 0, it is possible for the covariate to have zero affect on the response variable. When ranges do not include zero, it can be concluded that 95 percent of the time, a test will conclude the covariate has an effect on the response, even if it is higher or lower than the defined coefficient value.\\

    \item Reproduce the least squares estimate of $\sigma^2$. \\

\textbf{Explanation:}\\ 
The folowing formulas are a walk through of how to calculate the Least Squares Estimate (LSE) of $\sigma^2$ AKA Residual Standard Error(RSE) by using the Residual Sum of Squares (RSS) and degrees of freedom(df).\\

\textbf{Calculating RSE:}
\begin{align}
\nonumber
&\text{RSE} = \sqrt{\text{RSS}/\text{df}}\\
\nonumber
&\text{RSS} = 36028.96 \\
\nonumber
&\text{df} = 1248\\
\nonumber
&\text{RSE} = \sqrt{36028.96/1248}\\
\nonumber
&\text{RSE} = 5.373
\nonumber
\end{align}  

This RSE or LSE is the same as given by the Model One summary output.\\
    
    \item Now use R to estimate both $R^2$ and $R^2_{adj}$ with the results of model output from part 1. Interpret  the results. \\

\textbf{Explanation:}\\ 
From the summary information, we see that $R^2$ is 0.3178 and $R^2_{adj}$ is 0.3085. These R-squared values tell us a lot about the accuracy of the current model. With an adjusted R-squared of 0.3085, it is reasonable to say that we are missing out on a lot of information that this data could be giving us. We are losing almost 70 percent accuracy with the current model. This encourages me to rework the model to find the covariates that will give us the highest amount of prediction precision.\\

    \item Use backward selection by means of the stepAIC function from the library MASS to find the best model according to AIC.\\
\textbf{Explanation:}\\     
According to the AIC step function conducted in R, the best model includes: temperature, branch, day, driver, bill, and number of pizzas. This was supported by the p-values and confidence intervals found for the first model. These covariates were the only ones that held significant values. Therefore, it makes perfect sense that they would show up in the best fit multiple regression model.\\
    
    \item Obtain $R^2_{adj}$ from the model identified in 5. and compare it to the full model from 1.\\

\textbf{Explanation:}\\ 
The $R^2_{adj}$ barely changed in Model Two compared  to Model One. it only increased by 0.0007. That is minuscule and indicates this model has the same level of accuracy as the first model. I think that to get a model with a greater $R^2_{adj}$, you would have to specify which driver and which branch to include in your prediction.

  \item Identify whether the model assumptions are satisfied or not. \\
    
\textbf{Assumptions Check:}

\textbf{Linearity:}
Given that the F-test in the summary output of Model Two indicates that the included covariates are helpful in the model, I would subjectively say that this data meets the assumption of linearity. Because using the full model compared to the reduced does indicate a slight linear relationship. If it had no impact, you would simply drop on or more of the covariates.\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{1. Assumption Check for Constant Variation and Normality Model Two.png}
    \caption{Assumption Check for Constant Variation and Normality Model Two}
    \label{fig:enter-label}
\end{figure}


\textbf{Normality:}
By using the lillie.test in R and observing the Q-Q plot in Figure 0.1, a conclusion can be made about the data's normality- it is not normal. The P-value associated with the lillie.test is well below 5 percent meaning that we reject the null. The Q-Q plot indicates outliers towards the end and beginning of the observations. This implies non-normality of the data.

\textbf{Constant Variance:}
The Residuals vs Fitted plot in Figure 0.1 shows an curved variation of the residuals. Towards the middle it levels out slightly but bends at the beginning and end of the observations. This means that the assumption for constant variation is not satisfied. This also supports the conclusion that the residuals diverge from normal.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{2. Assumption Check for Independency of Residuals Modeal Two.png}
    \caption{ACF Graph Model 2}
    \label{fig:enter-label}
\end{figure}


\textbf{Independence:}\\
\textbf{Of Residuals:}\\
The ACF function in Figure 0,2 tells us that the residuals are mostly independent. However, there are 3 bars around the 23 to 32 mark that slightly cross the blue-dotted line. This indicates that there could be some correlation between some of the residuals. This is a similar finding to the independence of the covariates. They are practically independent, but there are a few that need to be checked for overlap. With this being said, I will conservatively conclude that the residuals are not independent.

\textbf{Of Covariates:}\\
Because some of the covariates are numerical and some are categorical, there are seperate checks that we have to run. For the numerical covariates alone, it was simply to put together a correlation chart and use that to visualize any possible correlations.

\textbf{Numerical Variable Check:}\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{3. Correlation Matrix of Key Numeric Variables.png}
    \caption{Correlation Matrix of Key Numeric Variables}
    \label{fig:enter-label}
\end{figure}

Figure 0.3 Correlation Matrix tells us so much about the numerical covariates (temperature, bill, and pizzas) and their independence or lack there of. There is a slight negative correlation between temperature and bill, and also temperature and pizzas. Probably for the same reason: more pizzas (and higher price) means a larger order which implies a longer prep time leading to more opportunity for pizza to lose heat. There is a pretty strong correlation between bill and pizzas. This makes sense because you spend more money on each pizza that you add to your order. This correlation might cause overemphasis of certain information. Because of these things, I conclude that the numerical covariates are not independent, at least not all of them.

\textbf{Categorical Variable Check:}\\
For the categorical variables (day, branch, and driver) I ran a correlation test comparing each of them with the other using a $\chi^2$ test. Using a Cramer V correlation value, we can see if each of the variables have correlation with the other. 

\textbf{Branch v. Driver:}
\begin{align}
\nonumber
&\text{Correlation Value} \geq 0.30 \text{ indicates moderate to extreme correlation}\\
\nonumber
&\text{Correlation Value} < 0.30 \text{ indicates weak to nonexistent correlation}\\
\nonumber
&\textbf{Correlation Value} = 0.1498\\
\nonumber
&\text{Results:} \text{ The value indicates a weak positive correlation between the covariates branch and driver.}\\
\nonumber
\end{align} 

\textbf{Branch v. Day:}
\begin{align}
\nonumber
&\text{Correlation Value} \geq 0.30 \text{ indicates moderate to extreme correlation}\\
\nonumber
&\text{Correlation Value} < 0.30 \text{ indicates weak to nonexistent correlation}\\
\nonumber
&\textbf{Correlation Value} = 0.0680\\
\nonumber
&\text{Results:} \text{ This value indicates nonexistent correlation AKA independence between branch and day.}\\
\nonumber
\end{align}  

\textbf{Day v. Driver:}
\begin{align}
\nonumber
&\text{Correlation Value} \geq 0.30 \text{ indicates moderate to extreme correlation}\\
\nonumber
&\text{Correlation Value} < 0.30 \text{ indicates weak to nonexistent correlation}\\
\nonumber
&\textbf{Correlation Value} = 0.3532\\
\nonumber
&\text{Results:} \text{ The value indicates a moderate level of correlation between the covariates day and driver.}\\
\nonumber
\end{align} 
Based on these test results, we can conclude that, similar to the residuals, most of these covariates are independent of each other but there is some overlap. Therefore, we must conclude that these covariates do not satisfy the assumption of independent covariates.\\

\textbf{Assumptions Summary:}
\begin{align}
\nonumber
&\text{Linearity} = \text{TRUE}\\
\nonumber
&\text{Normality} = \text{FALSE}\\
\nonumber
&\text{Constant Variance} = \text{FALSE}\\
\nonumber
&\text{Independence:} \\
\nonumber
&\text{Of Residuals}= \text{FALSE}\\
\nonumber
&\text{Of Covariates}= \text{FALSE}\\
\nonumber
\end{align} 

    


    
    \item Are all variables from the model in 5. causing the delivery time to be either delayed or improved? \\ 
\textbf{Explanation:}\\
Temperature, Domenico the driver, and the East branch cause the delivery time to be shortened (aka improve). 
Whereas the bill and number of pizzas cause the order time to increase (aka delay). This is gathered from the coefficients listed on page 3 of this report. 
\\
    
    \item Test whether it is useful to add a quadratic polynomial of temperature to the model. \\

\textbf{Explanation:}\\
Using R to plug in the squared version of temperature into a new model allowed us to determine whether or not a quadratic polynomial of temperature was helpful or not in making the model more precise. The summary shows an increase in the adjusted R-squared which immediately makes this a better model than our previous one. This means that keeping the quadratic transformation of temperature in the model helps with its predictive power. This quadratic transformation being helpful also allows us to say that the relationship between time and temperature is not strictly linear (because a curved shape provides a better fit). This is further supported by the results of the Anova Test: a significantly low p-value that indicates the quadratic model is the superior choice.\\
    
    \item Use the model identified in 5. to predict the delivery time of the last captured delivery (i.e. number 1266). Use the predict() command to ease the calculation of the prediction. \\

\textbf{Explanation:}\\
Using the last captured delivery as our "new data" in the predict function gave us the estimated time of delivery strictly based on the model we constructed. The calculation and interpretation are as follows:

\textbf{Prediction Output and Percent Error:}
\begin{align}
\nonumber
&\text{Predicted Delivery Time} = 34.2296 \text{ minutes}\\
\nonumber
&\text{Actual Delivery} = 35.7375 \text{ minutes}\\
\nonumber
&\text{Percent Error} = \frac{\text{Predicted-Actual}}{\text{Actual}}\\
\nonumber
&\text{Percent Error} = 0.0422, 4.22 \text{ percent}\\
\nonumber
\end{align}  

\textbf{Interpretation:}\\    
Using the latest model, we predict the time of the latest delivery to be 34.2296 minutes. There is a percent error of 4.22 percent in this prediction. I am not sure of its significance based on context, but it would not come as a shock to me if this percent error was significant because of the low value of the $R^2_{adj}$ of our model.
\end{enumerate}


\newpage

\textbf{\underline{APPENDIX}}\\
\textbf{R code:}\\

    \begin{lstlisting}[language=R]
#### HW 2 ####
data <- read.csv("pizza_delivery.csv",header = TRUE)
data
View(data)
#### 1 Model One ####
m1 <- lm(time~ temperature + branch + day + operator + driver + bill + pizzas + discount_customer, data = data)
m1

summary(m1)

#Using the summary output, it is easy to see the significance of some of these coefficients in predicting pizza delivery time.
#Whenever the covariate is increased by one unit, its corresponding coefficient gives the increase(+) or decrease(-) that will be seen in pizza delivery time.
#There are 5 coefficients that are extremely significant in predicting the response variable: temperature, branchEast, driverDomenico, bill, and number of pizzas.
#According to the p-values (which are also significantly lower than and alpha of 0.05), these variables are important in this linear regression model.
#This is also supported by the p-value result of the F-test. Having such a low p-value indicates that the full model (as compared to the reduced model) is a better predictor for the response variable.
#What this means is that the covariates are important and at least some of them should be kept in the model to allow a more accurate prediction.

#### 2 ####
confint(m1, level = 0.95)  # 95% CI

#These confidence intervals also give us an indication of the most significant variables. All 5 that showed significant p-values have a range that is fully positive or fully negative.
#When a confidence interval includes 0, this means that it is possible for that covariate to have zero affect on the response variable. When ranges do not include zero,
#it can be concluded that 95% of the time, a test will conclude the covariate has an effect on the response, even if it is higher or lower than the defined coefficient value.

#### 3 ####
#Least Squares Estimate of sigma^2 AKA Residual Standard Error is calculated by the ollowing formula:
#Sqrt(Residual sum of squares/degrees of freedom)

#To calculate RSS, we have to square the residuals and then sum them which is produced by the following code:

residuals <- resid(m1) #residuals
RSS <- sum(residuals^2) #squaring residuals and then summing them to produce RSS

df <- m1$df.residual #retrieve degrees of freedom from the model

LSE <- sqrt(RSS / df) #calculating Least Squares Estimate (LSE)
LSE

#This gives us 5.373 which matches the Residual Standard Error that the summary output gave us.

#### 4 ####
#From the summary information, we see that R-squared is 0.3178 and Ajusted R-squared is 0.3085.
#These R-squared values tell us a lot about the accuracy of the current model. With an adjusted R-squared of 0.3085,
#it is reasonable to say that we are missing out on a lot of information that this data could be giving us. 
#We are losing almost 70 percent accuracy with this current model. This encourages me to rework the model to find the 
#covariates that will give us the highest amount of prediction precision. 

#### 5 ####
library(MASS)
step <- stepAIC(m1, direction="backward")
step$anova 

#According to AIC, the best model includes: temperature, branch, day, driver, bill, and number of pizzas.

#### 6 Model Two ####
m2 <- lm(time~ temperature + branch + day + driver + bill + pizzas, data = data)
m2

summary(m2)

#The adjusted  R-squared barrely changed in Model Two compared  to Model One. it only increased by 0.0007.
#That is minuscule  and indicates the same level of accuracy that is lost in this model.
#I think that to get a model with a greater Adjusted  R-squared, you would have to specify which driver and which branch to include in your prediction.

#### 7 Checking Assumptions ####
###Checking for normality of residuals (which will apply to the normality of the response variable).
install.packages("nortest")
library(nortest)

m2$residuals
lillie.test(m2$residuals)
#P-value is well below 5% meaning that we reject the null. 
#In this case, it means that the residuals vary significantly from normality.
#It does not meet the normality assumption.

###Checking for constant variation
mean(m2$residuals)
par(mfrow=c(2,2))
plot(m2)
#The Residuals vs Fitted plot shows an increasing variation. This means that the assumption for 
#constant variation is not met. This also supports the conclusion that the residuals diverge from normal.
#The Q-Q plot matches the result of the lillie.test- not normal.
#The plot indicates outliers towards the end and beginning of the observations.

###Checking for independence of residuals
acf(m2$residuals)
#The acf function tells us that the residuals are mostly independent.However, there are 3 bars that slightly 
#cross the blue-dotted line. This indicates that there could be some correlation between a few of the residuals. 
#This is a similar finding to the independency of the covariates. They are moslty independent, but there are a few 
#that need to be checked for overlap.

### Checking for independence of covariates

##Numerical covariate independence check
install.packages("PerformanceAnalytics")
library(PerformanceAnalytics)

nums <- data[, c("temperature", "bill", "pizzas")]

chart.Correlation(nums, method = "spearman", histogram = TRUE, pch = 16, main = "Correlation Matrix of Key Variables")
#This correlation chart tells us so much about the numerical covariates and their independence or lack there of.
#There is a slight negative correlation between temperature and bill, and also temperature and pizzas. Probably 
#for the same reason- more pizzas (higher price also) means a larger order -> longer prep time -> more time for pizza to lose heat.
#There is a pretty strong correlation between bill and pizzas. This makes sense because you spend more money on each pizza.
#This correlation might cause overemphasis of certain information.

##Categorical covariate independence check
branch.v.driver <- table(data$branch, data$driver)
chisq.test(branch.v.driver)                    
DescTools::CramerV(branch.v.driver) 

#Result:0.1498
#Interpretation: The correlation value indicates a weak positive correlation between the covariates branch and driver. 

branch.v.day <- table(data$branch, data$day)
chisq.test(branch.v.day)                    
DescTools::CramerV(branch.v.day) 

#Result:0.0680
#Interpretation: The correlation value indicates an independency between the covariates branch and day.

day.v.driver <- table(data$day, data$driver)
chisq.test(day.v.driver)                    
DescTools::CramerV(day.v.driver) 

#Result:0.3532
#Interpretation: The correlation value indicates a moderate level of correlation between the covariates day and driver.


#Based on these observations of independence or lack there of, we can conclude that there are some covariates that 
#we should consider merging or not including due to correlation. This could possibly help us reduce the danger of overemphasizing 
#certain information and making a more reliant model.

##Checking for continuous response variable
class(data$time)
#The outcome of this function is "numeric" which most likely indicates a continuous variable. 

#Overall, the assumptions of normality, independence of residuals, independence of covariates and constant variation
#are unmet, possibly met, unmet, and unmet respectively.

#### 8 ####
#Temperature, Domenico the driver, and the East branch cause the delivery time to be shortened (aka improved). 
#Whereas the bill and number of pizzas cause the order time to increase (aka delayed).

#### 9 ####
m_quad <- lm(time ~ temperature + I(temperature^2) + branch + day + driver + bill + pizzas, data = data)
m_quad

summary(m_quad)
#This summary shows an increase in the adjusted R-squared which immediately makes this a better model than our previous one. 
#This means that keeping the quadratic transformation of temperature in the model helps with its predictive power. 
#This quadratic transformation being helpful also allows us to say that the relationship between time and temperature is not strictly linear
#(because a curved shape provides a better fit)

anova(m2, m_quad)   # if p < 0.05, the quadratic term significantly improves fit
#This better fit is also supported in the results of the anova test: significantly low p-value that indicates the quad model is the superior choice.


#### 10 ####
last.captured.delivery <- data[nrow(data), ]   
View(last.captured.delivery)

predict(m1, newdata = latest_delivery)

#Result: 34.2296 minutes

#Points of comparison
#Actual time of last delivery: 35.7375

mean(data$time)
#Result: 36.5063 minutes

#Difference between actual and predicted: 1.5079
#Percent Error: 1.5079/35.7375 = 0.0422 aka 4.22%

#Interpretation: Using our latest model, we predict the time of the latest delivery to be 34.2296 minutes. 
#There is a percent error of 4.22 percent in this prediction. I am not sure of its significance based on context, but
#it would not come as a shock to me if this percent error was significant because of the 
#low value of the adjusted R-squared of our model.

    \end{lstlisting}


\end{document}
