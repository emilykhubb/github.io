%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage{enumerate}
\usepackage{graphicx}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
    \lstset{language=R}
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm,amssymb} % Math packages
\usepackage{graphicx}
\usepackage{url}
\usepackage{tabto}

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\newcommand{\argmax}{\arg\!\max}
\title{	
\normalfont \normalsize 
\textsc{Mississippi State University} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Data Analysis One  \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Emily Hubbard} % Your name

\date{\normalsize\text{October 23, 2025}} % Today's date or a custom date

\begin{document}

\maketitle % Print the title

%----------------------------------------------------------------------------------------
%	PROBLEM 1
%----------------------------------------------------------------------------------------

\section*{HW 5}
%\lipsum[2] % Dummy text

\textbf{Executive Summary:}\\

\textbf{A. Introduction:}

\qquad Understanding what drives consumer brand choice helps predict market behavior. For example: Do discounts on a particular brand increase purchase likelihood? Does brand loyalty outweigh price sensitivity? How does changing the list price influence the final decision? Using the OJ dataset in R, we analyze which factors most influence a customer’s choice between Minute Maid (MM) and Citrus Hill (CH). We will fit multiple machine-learning methods and compare their test data performance:
\begin{enumerate}[1.]
    \item Unpruned Decision Tree
    \item Pruned Decision Tree
    \item Bagging (aggregation of trees)
    \item Random Forests
    \item Boosting Tree Method
    \item Support Vetor Machines (SVMs) - Linear
    \item Support Vetor Machines (SVMs) - Non-linear
\end{enumerate}

\qquad For each model, we will train on a portion of the data and test on the remaining observations. A confusion matrix, accuracy and misclassification rates, and interpretations will be produced for each model. After fitting and testing all models, we will use comparative tools to evaluate which model does the best job of predicting the classification of the binary response variable Purchase (CH vs. MM).\\

The question is: Among these models, which will produce the greatest prediction power?\\

\textbf{B. Data Collection}

\begin{itemize}
\item \textbf{"Purchase"} A factor with levels CH and MM indicating whether the customer purchased Citrus Hill or Minute Maid Orange Juice. 
\item \textbf{"WeekofPurchase"} Week of Purchase
\item \textbf{"StoreID"} Store ID
\item \textbf{"PriceCH"} Price charged for CH
\item \textbf{"PriceMM"} Price charged for MM
\item \textbf{"DiscCH"} Discount offered for CH
\item \textbf{"DiscMM"} Discount offered for MM
\item \textbf{"SpecialCH"} Indicator of special on CH
\item \textbf{"SpecialMM"} Indicator of special on MM
\item \textbf{"LoyalCH"} Customer brand loyalty for CH
\item \textbf{"SalePriceMM"} Sale price for MM
\item \textbf{"SalePriceCH"} Sale price for CH
\item \textbf{"PriceDiff"} Sale price of MM less sale price of CH
\item \textbf{"Store7"} A factor with levels No and Yes indicating whether the sale is at Store 7.
\item \textbf{"PctDiscMM"} Percentage discount for MM
\item \textbf{"PctDiscCH"} Percentage discount for CH
\item \textbf{"ListPriceDiff"} List price of MM less list price of CH
\item \textbf{"STORE"} Which of 5 possible stores the sale occured at
\end{itemize}


\textbf{C. Summary Information}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{1.2 Purchase Outcomes .pdf}
    \caption{Visualization of OJ Data Binary Response Variable}
    \label{fig:enter-label}
\end{figure}

This figure visualizes the proportion of the response variables that fall in the "CM" category versus the "MM" category.\\

\textbf{Correlation Chart for OJ Numerical Data:}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{2. Correlation Chart.pdf}
    \caption{Correlation Chart}
    \label{fig:enter-label}
\end{figure}
\textbf{Observations:}

\qquad This plot is busy, but the key takeaway is the many high correlations. There are 15 correlation coefficients that are greater than $\lvert0.5\rvert$ which indicates substantial multicollinearity across the variables in the data set. Tree-based models handle multicollinearity really well. Instead of trying to use every overlapping variable at once, a tree just picks one at a time and mostly ignores the duplicates. When you use a collection of models (bagging, random forests, boosting), this gets even better: different trees may pick different collinear variables and averaging their decisions decreases variance. In short, tree methods won't get confused. They will pick what’s useful, skip the redundancy, and still make solid predictions. This is why these methods will be good to use on the OJ data set. SVMs will also be a useful tool to draw the “best possible line” (or boundary) that separates the two classes with the biggest safety margin between them. Because of this margin maximization, SVMs also handle multicollinearity well and will be a useful tool in analyzing this data set. \\

Before we get started, I want to define the binary response variable:

\[
Y = 
\begin{cases}
0 & \text{if Citrus Hill is purchased}\\
1 & \text{if Minute Maid is purchased}\\
\end{cases}
\]\\

\textbf{D. Report Body}
\begin{enumerate} [1.]
    \item  \textbf{Unpruned Decision Tree} \\

To begin, we will create a decision tree without any pruning, leaving  the tree with all of its terminal nodes. 

The following is the decision tree in text form. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{DA HW 5/2. Unpruned Tree Text Output.png}
    \caption{Unpruned Tree Text Output}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
We can interpret each terminal node by looking at the previous nodes it falls under. For example, when looking at terminal node 11, we start at the node 2. This node states that these consumers have below average (< 0.50) loyalty to CH and are more likely to choose MM. This node is then broken down into customers with loyalty less than or greater than 0.28. Our terminal node falls underneath the customers with slightly more brand loyalty (> 0.28). This node states that even if CH has a slightly higher price (> 5 cents), 60 percent of the customers would still choose to purchase CH. This node does have a fairly high deviance of 147. This indicates a higher impurity in this node and a greater error rate when looking at this node alone.

Looking at a visual representation like the figure below is much simpler to interpret.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/3. Tree Plot.pdf}
    \caption{Unpruned Tree}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
With this visual we can clearly count 9 terminal nodes. The more nodes we have, the more difficult to interpret. This is why it is important to consider pruning the decision tree, creating fewer terminal nodes with less complex leaves.

Below, on the left hand side, is a summary output of the model, giving us relevant information and informing us on the training misclassification rate. To the right, we have the decision matrix which allows us to see where our model went wrong. It also allows us to caluculate the accuracy and miscalssification rates of the model on our test data.\\

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.73\textwidth}
    \includegraphics[width=\textwidth]{DA HW 5/1. Summary of Unpruned Tree.png}
    \caption{Unpruned Tree Summary Output}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.22\textwidth}
    \includegraphics[width=\textwidth]{DA HW 5/5. Confusion Matrix.png}
    \caption{Unpruned Tree CM}
  \end{minipage}
\end{figure} 
\textbf{Explanation:}\\
It is important to not that this model only used 5 of the 17 covariates to produce this tree. As mentioned before, this is helpful in reducing multicollinearity effects because the decision tree is calculated using only the most relevant covariates at a given decision point.

The summary output also reports a training misclassification rate of 15.88 percent. 

On test data, according to the confusion matrix, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8296\\
\nonumber
&\text{Misclassification Rate} = 0.1704
\nonumber
\end{align}

Overall, this model does a fair job at predicting the appropriate classification for the response variable with an accuracy of approximately 83 percent.\\

Now, we will find out if we can increase the accuracy of the decision tree by finding the optimal number of nodes for this data and pruning our decision tree accordingly. 

    \item  \textbf{Pruned Decision Tree} \\

To find the correct size of the tree, a cross-validation must be preformed to see which size corresponds to the lowest deviance. The following graph shows us number of nodes in relation to magnitude of deviance.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\linewidth]{DA HW 5/6. Cross-Validation to Find Optimal Number of Nodes.pdf}
    \caption{Cross-Validation to Find Optimal Number of Nodes}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
Based on this graph as well as an R function that extracts the exact number of nodes that corresponds to the lowest deviance, the best tree size has 9 terminal nodes. This means that, in this case, the lowest deviance actually occurs with an unpruned tree. Because of this, for the sake of comparison, we will create a model with the best tree size set to 5 nodes. 

The following model is the visual representation of a tree with 5 terminal nodes:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/8. Pruned Tree Model.pdf}
    \caption{Pruned Tree}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
By visualizing alone, it is easy to see why it would be easier to interpret. It is less crowded and has fewer decision splits. However, ease of interpretability does not always equal greatest predictive power. We will see this as we analyze the tree in the following outputs.

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.60\textwidth}
    \includegraphics[width=\textwidth]{DA HW 5/7. Pruned Tree Summary Output.png}
    \caption{Pruned Tree Summary Output}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{DA HW 5/12. Confusion Matric for Pruned Tree.png}
    \caption{Pruned Tree CM}
  \end{minipage}
\end{figure} 
\textbf{Explanation:}\\
Compared to the unpruned tree, this model used 3 less covariates for construction, meaning only 2 covariates were considered of the 17 covariates to start.

The summary output reports a training misclassification rate of 20.50 percent. This makes sense given that 9 terminal nodes is optimal. This tree has less nodes with fewer decision points and in this case, it led to a higher error rate in the training set.

On test data specifically, according to the confusion matrix, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8037\\
\nonumber
&\text{Misclassification Rate} = 0.1962
\nonumber
\end{align}

This model has an approximate 3 percent higher misclassification rate than that of the unpruned decision tree. Looking at these two models alone, it is clear that the 9 node model is superior.\\ 

We will now look at a few visuals and reemphasize important statistics that allow us to make a decision on which model to superior

\textbf{Comparison of Unpruned vs Pruned Tree Models:}\\

Models:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/9. Comparison of Unpruned vs Pruned Model.pdf}
    \caption{Comparison of Unpruned vs Pruned Tree}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
Visually, the tree with fewer nodes is more appealing but statistically less powerful. The training error rate within these trees as well as the difference in the two is as follows:

\begin{align}
\nonumber
&\text{Training Error Rate (Unpruned)} = 0.1588\\
\nonumber
&\text{Training Error Rate (Pruned)} = 0.2050\\
\nonumber
&\text{Difference} = 0.0462
\nonumber
\end{align}
The pruned tree has a training error rate 4.62 percent higher than the unpruned tree.

Matrices:
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/11. Comparison of Confusion Matrices.pdf}
    \caption{Comparison of Confusion Matrices}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
Ther are a few small things to note on these visualized confusion matrices. One, the lighter the shade that closer the precidcted values were to the actualy values (lower deviance). This means that the lighter blue color should give us higher confidence in the unpruned model. Two, the pruned tree misclassifies MM purchases more often seen by the larger red tile on the top right hand side. 
\begin{align}
\nonumber
&\text{Testing Error Rate (Unpruned)} = 0.1704\\
\nonumber
&\text{Testing Error Rate (Pruned)} = 0.1962\\
\nonumber
&\text{Difference} = 0.0258
\nonumber
\end{align}
The pruned tree also has a 2.58 percent higher error rate on the test data. 

Both these visuals and error rates help to confirm the superiority of the 9 terminal node model. 

Now, we can see if other machine-learning methods create a more reliable model. We will start with bagging which entails compiling many trees and taking an average of their classification votes.

    \item  \textbf{Bagging (aggregation of trees)}\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/13. Output for Bag Model.png}
    \caption{Output for Bag Model}
    \label{fig:enter-label}
\end{figure} 

\textbf{Explanation:}\\
One important thing to note is that bagging considers all 17 variables at each split. This can sometimes cause problems with multicollinearity. Averaging 500 total trees can help with this multicollinearity affect, but I suspect this one will not beat the unpruned tree model. The training error rates are as follows:
\begin{align}
\nonumber
&\text{CH Class Error Rate (Unpruned)} = 0.1752\\
\nonumber
&\text{MM Class Error Rate (Pruned)} = 0.2571\\
\nonumber
&\text{Average or OOB Estimate Error Rate} = 0.2075
\nonumber
\end{align}

This training rate is higher than both previous training rates. However, the testing misclassification rates hold more importance when it comes to model reliability, so we will take a look at that next in the below confusion matrix.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.30\linewidth]{DA HW 5/13.1 Confusion Matrix for Bag Modeal.png}
    \caption{Confusion Matrix for Bagging Model}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
On test data, according to this confusion matrix, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8185\\
\nonumber
&\text{Misclassification Rate} = 0.1815\\
\nonumber
\end{align}
Thought it holds a lower accuracy compared to the unpruned model, the bagging method did perform better than the pruned model.

Now, we will take a look at Random Forests method. This method uses less variables in each decision split. This allows it to better deal with multicollinearity. Therefore, I believe it will perform better than the bagging method. Let's take a look.

    \item  \textbf{Random Forests}\\
The following summary gives an overview of the random forest model created in R.    

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/14. Output for RF Model.png}
    \caption{Output for RF Model}
    \label{fig:enter-label}
\end{figure} 

\textbf{Explanation:}\\
So, the number of variables that was tried at each decision split was 4. This is approximately the $\sqrt{17}$. This is typical for the random forest method when dealing with classification. This is what will allow a greater handle on multicollinearity.

\begin{align}
\nonumber
&\text{CH Class Error Rate (Unpruned)} = 0.1588\\
\nonumber
&\text{MM Class Error Rate (Pruned)} = 0.2603\\
\nonumber
&\text{Average or OOB Estimate Error Rate} = 0.19.88
\nonumber
\end{align}
As suspected, the random forest method creates a slightly lower training misclassification rate compared to bagging.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.30\linewidth]{DA HW 5/14.1 Confusion Matrix for RF Model.png}
    \caption{Confusion Matrix for RF Model}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
On test data, according to the confusion matrix above, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8259\\
\nonumber
&\text{Misclassification Rate} = 0.1741\\
\nonumber
\end{align}
This is the second best model yet, just slightly underneath the unpruned accuracy rate. As I suspected, this method was superior to the bagging method.

Now, we will look at a more unique method called Boosting. 

    \item  \textbf{Boosting Tree Method}\\
This method creates a multitude of smaller treees, each trying to correct the mistakes of the previous tree. The following is the summary output for the boosting model. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/15. Output for Boost Model.png}
    \caption{Output for Boost Model}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
This model filtered out 2 of the 17 models, leaving 15 that were used in its construction. 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{DA HW 5/15.1 Confustion Matrix for Boost Model.png}
    \caption{Confusion Matrix for RF Model}
    \label{fig:enter-label}
\end{figure} 

\textbf{Explanation:}\\
On test data, according to the above confusion matrix, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8296\\
\nonumber
&\text{Misclassification Rate} = 0.1704\\
\nonumber
\end{align}
This model is now tied with the unpruned decision tree for level of accuracy. It outperformed both bagging and random forests methods.

Moving away from decision tree models, we are also going to build some models based on Support Vectors Machines.

    \item  \textbf{Support Vetor Machines (SVMs) - Linear}\\
SVMs have the goal to provide a hyperplane that can split the respnse space into nicely seperated classification areas. This allows the model to predict further data based on past support vectors. SVMs can be used linearly or non-linearly. It depends entirely on what the data will work better with.

The following is the summary output for the Linear SVM Model:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/16. Output for SVM Lineaar Model.png}
    \caption{Output for SVM Linear Model}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
The cost associated with the SVM is 0.1. This is the penalty associated with mistakes on the training data. It helps shape how wide or narrow the margin will be. There are shown to be 342 support vectors, or points that lie on or within the margin. These also include the misclassified data points. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.40\linewidth]{DA HW 5/16.1 Confusion Matrix for SVM Linear Model.png}
    \caption{Confusion Matrix for SVM Linear Model}
    \label{fig:enter-label}
\end{figure} 
\textbf{Explanation:}\\
On test data, according to the matrix above, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8370\\
\nonumber
&\text{Misclassification Rate} = 0.1630\\
\nonumber
\end{align}
This outperforms our very first method by 0.74 percent. 

Now we will see if a non-linear SVM can do as well. My assumption is no because if it works well with linear it is most likely going to perform well in a non-linear model.

    \item  \textbf{Support Vetor Machines (SVMs) - Non-linear}\\
Non-linear SVMs have two important factors, the cost and the gamma. We have discussed cost but not gamma. With non-linearity there is a curves and bumps and wiggles. Gamma determines how complex (snug to the data) these bumps are allowed to be.   

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{DA HW 5/17. Output for SVM Non-linear Model.png}
    \caption{Output for SVM Non-linear Model}
    \label{fig:enter-label}
\end{figure} 

\textbf{Explanation:}\\
The cost associated with this non-linear SVM is 10. The number of support vectors is 382. Without looking at the accuracy rate, it is already implied that this model underperforms compared to its linear counterpart. The more support vectors, the more likely there is less clear demarcation between classifications. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.40\linewidth]{DA HW 5/17.1 Confusion Matrix for SVM Non-linear Model.png}
    \caption{Confusion Matrix for SVM Non-linear Model}
    \label{fig:enter-label}
\end{figure} 

\textbf{Explanation:}\\
On test data, based on the matrix above, the model predicted the response values with the following amount of accuracy:
\begin{align}
\nonumber
&\text{Accuracy Rate} = 0.8037\\
\nonumber
&\text{Misclassification Rate} = 0.1963\\
\nonumber
\end{align}
As suspected this underperforms compared to most other models. 

Now that we have seen all of the models, as well as their accuracy and misclassification rates, let's conclude which model has the greatest prediction power.

    \item  \textbf{Overall Model Comparisons}\\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.30\linewidth]{DA HW 5/18. Modeal Accuracy Comparisons.png}
    \caption{Model Accuracy Comparisons}
    \label{fig:enter-label}
\end{figure} 

\textbf{Final Conclusion:}\\
Based on these accuracy rates, we have a clear winner on prediction power: Linear SVM.
Among all models, the Linear SVM achieved the highest test accuracy (0.837), edging out the tree-based methods (unpruned: 0.830; RF/boosting around 0.826–0.830). Based on this, we can infer that the pattern dividing CH and MM is almost a straight-line rule.

\end{enumerate}

\newpage

\textbf{\underline{APPENDIX}}\\
\textbf{R code:}\\

    \begin{lstlisting}[language=R]
#### HW 5 ####
#### 1. OJ Data and Package Installation ####
library(tree)
library(ISLR)
data(OJ)
View(OJ)
?OJ
summary(OJ)
str(OJ)

#### 2. Producing Training and Test Sets ####
set.seed (1)
sample.index <- sample (1:nrow(OJ), 800)
train <- OJ[sample.index,]
test <- OJ[-sample.index, ]

#### 3 and 4. Produce a tree output/summary ####
tree.OJ <- tree(Purchase~., data = train)
tree.OJ
summary(tree.OJ)
##Training Data Rates:
##Misclass: 0.1588

#### 5. Plot the Tree ####
plot(tree.OJ)
text(tree.OJ ,pretty =0)

#### 6. Predict Response and Create Confusion Matrix ####
yhat <- predict (tree.OJ ,newdata = test, type = "class")
OJ.test <- test$Purchase   

conf.mat.tree <- table(yhat, OJ.test)
conf.mat.tree
mosaicplot(conf.mat.tree, shade = TRUE, main = "Confusion Matrix (Unpruned Tree)")
accuracy.tree <- mean(yhat == OJ.test)
accuracy.tree
misclassification.rate.tree <- 1 - accuracy.tree
misclassification.rate.tree
## Test Data Rates:
##Accuracy: 0.8296
##Misclass: 0.1704

#### 7, 8 and 9. Producing a Plot and Finding Optimal Tree Size ####
set.seed(2)
cv.OJ <- cv.tree(tree.OJ, FUN = prune.misclass, K = 10)
cv.OJ
plot(cv.OJ$size ,cv.OJ$dev ,type="b",
main = "Cross-Validation to Find Optimal # of Nodes")

best.tree.index <- which.min(cv.OJ$dev)
best.tree.size <- cv.OJ$size[best.tree.index]
print(paste("The optimal tree size has", best.tree.size, "terminal nodes."))
##The optimal size is the same as the unpruned tree, so we will
# be using 5 terminal nodes instead.

#### 10. Pruned Tree Model ####
prune.OJ <- prune.tree(tree.OJ ,best = 5)
plot(prune.OJ)
text(prune.OJ ,pretty =0)
summary(prune.OJ)
##Training Data Rates:
##Misclass: 0.205


#### 11 ####
#The training misclassification error rate for the pruned tree
# is 4.62% higher than the one for the unpruned tree. 

#### 12 ####
#The test misclassification error rate for the pruned tree 
#is 2.58% higher than the unpruned tree.

#### Predict Response and Create Confusion Matrix for the Pruned Tree
yhat.prune <- predict (prune.OJ ,newdata = test, type = "class")
OJ.test.prune <- test$Purchase   

conf.mat.prune <- table(yhat.prune, OJ.test.prune)
conf.mat.prune
mosaicplot(conf.mat.prune, shade = TRUE, main = "Confusion Matrix (Pruned Tree)")
accuracy.prune <- mean(yhat.prune == OJ.test.prune)
accuracy.prune
misclassification.rate.prune <- 1 - accuracy.prune
misclassification.rate.prune
## Test Data Rates:
##Accuracy: 0.8037
##Misclass: 0.1962

#### 13 ####
library (randomForest)
?randomForest()

p <- ncol(OJ) - 1 
##Bagging:
set.seed(4)
bag.OJ <- randomForest(Purchase~.,data = train, mtry = p, importance =TRUE)
bag.OJ
##Training Data Rates:
##Misclass: 20.75%

yhat.bag <- predict (bag.OJ ,newdata = test, type = "class")
OJ.test.bag <- test$Purchase  

conf.mat.bag <- table(yhat.bag, OJ.test.bag)
conf.mat.bag
accuracy.bag <- mean(yhat.bag == OJ.test.bag)
accuracy.bag
misclassification.rate.bag <- 1 - accuracy.bag
misclassification.rate.bag
## Test Data Rates:
##Accuracy: 0.8185
##Misclass: 0.1815


##Random Forest:
set.seed(5)
rf.OJ <- randomForest(Purchase~.,data = train, mtry = floor(sqrt(p)),
importance =TRUE)
rf.OJ
##Training Data Rates:
##Misclass: 19.88%

yhat.rf <- predict (rf.OJ ,newdata = test, type = "class")
OJ.test.rf <- test$Purchase  

conf.mat.rf <- table(yhat.rf, OJ.test.rf)
conf.mat.rf
accuracy.rf <- mean(yhat.rf == OJ.test.rf)
accuracy.rf
misclassification.rate.rf <- 1 - accuracy.rf
misclassification.rate.rf
## Test Data Rates:
##Accuracy: 0.8259
##Misclass: 0.1741


##Boosting Method:
library (gbm)
train.bin.y <- ifelse(train$Purchase == "CH", 1, 0)
test.bin.y  <- ifelse(test$Purchase  == "CH", 1, 0)

class(test.bin.y)

set.seed (6)
boost.OJ <- gbm(train.bin.y~.-Purchase, 
            data = transform(train, train.bin.y = train.bin.y),
            distribution = "bernoulli", interaction.depth = 4)
boost.OJ
summary(boost.OJ)

best.n.trees <- gbm.perf(boost.OJ, method = "OOB", plot.it = FALSE)
best.n.trees

phat.boost <- predict(boost.OJ, newdata = transform(test, test.bin.y = test.bin.y), 
n.trees = best.n.trees, type = "response")
yhat.boost <- factor(ifelse(phat.boost > 0.5, "CH", "MM"),
levels = levels(test$Purchase))
OJ.test.boost <- test$Purchase

conf.mat.boost <- table(yhat.boost, OJ.test.boost)
conf.mat.boost
accuracy.boost <- mean(yhat.boost == OJ.test.boost)
accuracy.boost
misclassification.rate.boost <- 1 - accuracy.boost
misclassification.rate.boost
## Test Data Rates:
##Accuracy: 0.8296
##Misclass: 0.1704

#### SVM ####
install.packages("e1071")
library(e1071)
is.factor(train$Purchase)

##Linear SVM:
set.seed(1)
tune.lin.OJ <- tune(
  svm, Purchase ~ ., data = train,
  kernel = "linear",
  ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100))
)
best.lin.OJ <- tune.lin.OJ$best.model
best.lin.OJ
summary(best.lin.OJ)

yhat.svm.lin <- predict(best.lin.OJ, newdata = test)
OJ.test.svm.lin <- test$Purchase  

conf.mat.svm.lin <- table(yhat.svm.lin, OJ.test.svm.lin)
conf.mat.svm.lin
accuracy.svm.lin <- mean(yhat.svm.lin == OJ.test.svm.lin)
accuracy.svm.lin
misclassification.rate.svm.lin <- 1 - accuracy.svm.lin
misclassification.rate.svm.lin

##Non-linear SVM
set.seed(1)
tune.rad.OJ <- tune(
  svm, Purchase ~ ., data = train,
  kernel = "radial",
  ranges = list(
    cost  = c(0.1, 1, 10, 100, 1000),
    gamma = c(0.5, 1, 2, 3, 4)
  )
)
best.rad.OJ <- tune.rad.OJ$best.model
best.rad.OJ
summary(best.rad.OJ)

yhat.svm.rad <- predict(best.rad.OJ, newdata = test)
OJ.test.svm.rad <- test$Purchase  

conf.mat.svm.rad <- table(yhat.svm.rad, OJ.test.svm.rad)
conf.mat.svm.rad
accuracy.svm.rad <- mean(yhat.svm.rad == OJ.test.svm.rad)
accuracy.svm.rad
misclassification.rate.svm.rad <- 1 - accuracy.svm.rad
misclassification.rate.svm.rad


#### Compare Models Code ####
#Tree Plots:
par(mfrow=c(1,2))
plot(tree.OJ)
text(tree.OJ ,pretty =0)
plot(prune.OJ)
text(prune.OJ ,pretty =0)
#Confusion Matrices:
par(mfrow=c(1,2))
mosaicplot(conf.mat.tree, shade = TRUE, main = "Confusion Matrix (Unpruned Tree)")
mosaicplot(conf.mat.prune, shade = TRUE, main = "Confusion Matrix (Pruned Tree)")

#Results Table:
res <- data.frame(
  Model = c("Unpruned", "Pruned", "Bagged", "Random Forest", 
  "Boosted", "Linear SVM", "Non-linear SVM"),
  Accuracy = c(accuracy.tree,
               accuracy.prune,
               accuracy.bag,
               accuracy.rf,
               accuracy.boost,
               accuracy.svm.lin,
               accuracy.svm.rad)
)

res_desc <- res[order(res$Accuracy, decreasing = TRUE), ]
row.names(res_desc) <- NULL  # tidy row numbers
res_desc

#Code Check for Unpruned vs Boosted and Pruned vs Non-linear SVM:
identical(yhat, yhat.boost)#FALSE
sum(yhat != yhat.boost)#16 predictions are different

identical(yhat.prune, yhat.svm.rad)    #FALSE
sum(yhat.prune != yhat.svm.rad)   #50 predictions are different

#Because these pairs of models had the same accuracy, I wanted to make
#sure there wasn't an error in my coding. 

    \end{lstlisting}


\end{document}
